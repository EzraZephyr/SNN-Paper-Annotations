
## Sparse Spiking Gradient Descent

[论文链接](https://proceedings.neurips.cc/paper/2021/file/61f2585b0ebcf1f532c4d1ec9a7d51aa-Paper.pdf)

稀疏SNN梯度下降

因为正常训练的SNN通常依旧需要高消耗 但是训练后的神经元仅有1%是活跃的 也就是说浪费了99%的训练资源 所以作者提出了一种稀疏脉冲的SNN反向传播算法

首先使用正常的LIF进行前向传播 参考公式2
第一项和第二项为模型的泄露部分 衰减回原来的静息电位 第三项 使用l层第i个发送脉冲的神经元乘上i到j的突触权重 如果在l+1层的Sj也发放脉冲了就减去（Vth-Vreset）的电位作为上一层l输入的总影响

递归展开得到公式3 αt-k为衰减因子 时间步数相隔越远 衰减因子的影响越小 S[k]为判断是否发送脉冲

然后使用反向传播算法计算权重固定梯度 公式5 然后不让第三项参与到反向传播的计算 是因为是已知的固定值且是前向传播过程中计算并储存数据的 加上之后因为all-or-none的性质会导致准确性变差
公式5中的ε计算参考公式6 为计算脉冲又替代梯度近似的连续值 计算脉冲发生的变化对损失的影响

本文的重点 公式7 只有膜电位接近阈值时 神经元才会被激活 也就是只有活跃的神经元才会计算梯度 反之跳过

使用递归关系来优化计算 公式11
第一个分支 如果时间步在t+1 to t+n 这之间梯度为0 包含t+1 则证明这些时间步内不活跃 参考公式7 则可以直接通过未来活跃的 也就是t+n步的误差信号(因为t+n+m可能是活跃神经元) 直接用α衰减因子衰减过来
第二个分支 如果时间步在t+1 to t+n 这之间的梯度为0 但t+1开始梯度活跃 则需要计算时间步的误差贡献

Experiments省略
