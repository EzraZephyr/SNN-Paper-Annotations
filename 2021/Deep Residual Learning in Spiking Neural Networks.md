
## Deep Residual Learning in Spiking Neural Networks

[论文链接](https://proceedings.neurips.cc/paper/2021/file/afe434653a898da20044041262b3ac74-Paper.pdf)

残差连接版深度SNN

在普通的ANN版本的神经网络能比较容易实现恒等映射 比如说Y = ReLU(F(X) + X) 当F(X)为0时且X>=0的话Y = ReLU(X) = X是恒成立的 (此处省略了‘l'的层标识)所以可以进行残差连接 但是相对于用SN来替换ReLU的话 即使F(X)为0了 那么因为神经元模型的原因 也无法确保经过了SN()后的X 也就是SN(X)还能等于X 比如LIF模型 有一个多余的衰减因子τ影响映射 所以这里使用最简单的IF神经元模型来处理 因为没有衰减因子 所以可以做到当X[t]为1时 传入IF以可以使其发送脉冲 反之0的话不发送 这样用于SN()后可以使SN(X[t]) = X[t]以实现恒等映射

在之前的spiking resnet中忽略了脉冲的非线性特性 参考公式8 第二项因为是神经元01形式 所以可以替换为第三项 然后又因为S脉冲一定为1或0 而阈值一般又不能为0 所以这就导致梯度一直处于0和+∞的情况 导致梯度消失或者爆炸

公式9为提出的SEW块 参考table1 通过ADD和IAND作g函数 当A[t]为0时可以进行恒等映射 举例ADD 即O[t] = g(A[t], S[t]) = A[t] + S[t] =  0 + S[t] = S[t] 省略'l' (但是有疑问 当A[t] = 1时为2 是否有强制转换1的操作)同时 AND则可以设A[t] = 1 为恒等映射 参考公式10 输出都为常数 而非0或∞ 则缓解了梯度消失爆炸的问题 (引入替代梯度)