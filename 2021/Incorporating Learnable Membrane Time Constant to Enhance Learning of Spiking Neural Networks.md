
## Incorporating Learnable Membrane Time Constant to Enhance Learning of Spiking Neural Networks

[论文链接](https://arxiv.org/abs/2007.05785)

训练膜时间常数的SNN

作者发现 如图2所示 较小的τ会导致更快的充电并更快的衰减到静息电位 这也就变相的说明神经元变得更明白 在某种情况下减少τ等于变相的增大权重w 这两个同时调整可以达到1+1>2的效果

所以提出了PLIF神经元模型 遵循三个条件 膜时间常数τ在训练过程中自动优化 而不是一个固定的超参数 τ在同一层神经元中共享 不同层的神经元的τ不同

公式可以看作为标准的LIF神经元模型 参考公式5 但是直接训练τ可能会导致数值不稳定 因为τ处于分母 一旦小于1将很容易导致爆炸 所以对τ做了一些变化 参考公式6 用一个钳制函数k(a) 用sigmoid激活函数将k(α)限制在0-1之间 同时保证τ = 1/k(α) 这样可以限制τ一定处于一个大于1的区间 同时以RNN的写法 类似LSTM的记忆门与遗忘门 可以参考公式7  使用膜τ进行控制

输入并没有被转换为泊松编码 因为作者认为会引起“变异性” 所以直接输入到网络 而不需要先转换为脉冲

并且作者认为其他文章所写的max pooling会导致信息丢失是不正确的 因为maxpooling引用了赢家通吃机制 使得发放脉冲的神经元能够与下一层进行通信 忽略其他没有发放脉冲的神经元 因为神经元再次发送脉冲本就是困难的 如果使用平均池化可能使得下一部分脉冲得不到有效的电流以激活 所以使用最大池化直接输入1 可以让下一个被连接的神经元更容易被激活发放脉冲 增加拟合能力
