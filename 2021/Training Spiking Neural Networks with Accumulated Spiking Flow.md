
## Training Spiking Neural Networks with Accumulated Spiking Flow

[论文链接](https://ojs.aaai.org/index.php/AAAI/article/view/17236)

在传统的STBP中 使用的是一种双循环的策略 这种策略随着时间和深度的增加会导致计算复杂度变得越来越高 本文提出了一种以累计脉冲流 达到单循环完成的BP 节省计算复杂度

首先使用虚拟阈值Vth来定义损失函数 参考公式6 其中uTN表示输出层的膜电位向量

使用FI和FO来表示累计的输入电流总和和输出脉冲总和 可以理解为去掉时间维度 参考公式7

结合公式3 可以将FI重写为公式8 使用S作为尺度因子来表示FO和FI的比率 S= FO/FI 现在定义的是某一层的一个神经元中的FO和FI

然后因为FI^N是最终输出 所以不存在FO^N 只存在FO^N-1

关于为什么FI^N是最终输出：因为这篇文章遵循(Lee et all. 2020)的设定 输出层不产生任何脉冲 而是通过最终的膜电位向量来和损失函数进行结合(公式6) 可以避免脉冲的二值带来的非连续性

所以从第N-1层开始计算梯度 公式9 具体的链式法则过程 需要参考公式6和8进行 其中u  = mem 因为文中提到mem = FI 而FI = ∑x 这与膜电位的更新规则是一致的 同样 对于n<N-1 参考公式10 权重更新规则为公式11

在正常情况下 S = FO/FI ≈ 1/Vth 但是稀疏的输入会导致S不稳定 因为有些膜电位没有到达阈值的电流也会被FI所记录 导致S的值偏小  所以要把上文FO/FI对应的某一层的某一个神经元改为某一层的所有神经元来平衡S这个尺度因子 并且每隔几个周期进行更新来保持稳定 也就是S从某一层的某个神经元的尺度因子变为了某一层所有神经元的尺度因子