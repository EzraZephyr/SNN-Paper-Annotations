
## Optimal ANN-SNN Conversion for High-Accuracy and Ultra-Low-Latency Spiking Neural Networks

[论文链接](https://openreview.net/forum?id=7B3IJMM1k_M)

高精度低延迟的ANN转SNN

本文首先介绍了一下正常的IF模型 不含衰减项 参考公式2 然后使用"reset-by-subtraction"机制 即不直接重置为0以保留信息 参考公式3 4， 并使用阈值θ来代替发送的脉冲激活值 参考公式5

然后 将公式2和公式4结合得到公式6以获得膜电位的更新方程 并通过1-T进行求和 并使用ϕ(T)进行代替 具体查看公式7 - 公式8 这个公式为ANN-SNN转换的基本公式 通过SNN在T个时间步内的平均发放以拟合ANN的激活值 （因为SNN的输出是离散的 需要通过平均发放的方式来拟合成一个线性 适应ANN的线性输入）

假设膜电位初始值为0 那么公式8的最后一项就为- vl(T) / T 如果只有前一层的贡献 那么将会忽略膜电位 脉冲发放等影响 使得无法记录脉冲发放导致膜电位一直累计 所以这一项是必须存在的 所以需要一个较大的T来稳定这些变化 这也是本文解决的主要瓶颈

提出了三个误差

第一个为剪裁误差
参考公式10 其中λ为ANN输出的激活值的99.99%最大值 关于选取99.99%是为了排除计算错误的极大值导致网络不稳定
首先内层部分为alT/λ 将ANN的激活值对于时间步T和最大值λ进行缩放 再使用外侧的θ/T调整为SNN的平均值 最后剪裁到0-阈值之间 形成clipping error

第二个为量化误差
因为脉冲只能为0和1 而平均活动又通过一个固定值θ进行控制 参考公式7第三部分 所以ANN的激活值只能取到平均活动的倍数 也就在转换过程中必定会存在量化误差

第三个为不均匀误差
因为ANN的输出是连续且固定的变换 但是SNN的输出是离散的脉冲时态 与输出的分布有关 这是如果脉冲集中在早期时间步发放 就会导致整体的脉冲因为膜电位早期累计而变多 反之变少 导致分布不均匀形成误差 关于文中说的 如果膜电位落在[0, θ]中则不会形成不均匀误差 理解为如果膜电位在0到阈值之间 则说明不足以形成脉冲 所以膜电位趋于稳定 则不存在不均匀误差

关于公式11 12的注释已经在附件注明

公式14为转换误差为0的证明 设当初始膜电位为0 量化步长L和时间步长T相等 同时同样使用λ控制最大输出 那么公式11就可以无损替换为公式13 所以他们的差为0 ANN-SNN的转换误差为0

关于移位1/2会让在不同T和L之间的转换误差依旧为0的证明位于 附录Proof of Theorem 2 详细 use direct proof method

然后在训练的时候 因为包含取整操作 导数在整数点上不可微 需要用直通估计器近似取整函数的导数为1 参考公式17