 
## IM-Loss: Information Maximization Loss for Spiking Neural Networks

[论文链接](https://proceedings.neurips.cc/paper_files/paper/2022/file/010c5ba0cafc743fece8be02e7adb8dd-Paper-Conference.pdf)

提出了一种对于SNN的  新的损失函数 IM-Loss

3先验知识 使用LIF模型 使用硬重置 并且给出了离散形式和迭代形式 迭代版本的更便于在通用机器学习编程框架上实现 公式1到6

并且在反向传播中使用STBP进行计算 这就一定需要计算到不可微的脉冲 所以需要使用代理梯度 但是传统的代理梯度SG会限制网络的学习能力 所以这里作者引入了一种进化的SG(ESG)让其在训练早期保持较强的能力 并且在训练末期逐渐接近准确的梯度

4.1首先在前向传播中 膜电位需要量化为0/1脉冲 这会带来信息损失 所以如果想要SNN更接近ANN的性能 需要将脉冲张量O尽可能的多反映U的信息流 需要将互信息最大化 公式9
其中H(O)为O的信息熵 反应了不确定性 当O完全确定时 H(O)为0 当O完全不确定时 即概率为0.5 则H(O)为1 最大值
同样H(O|U)为条件熵 表面当U确定的时候 O还有多少不确定性 如果O完全由U决定 则证明O也是确定的 H(O|U)为0 反之为H(O)
在SNN的背景下 H(O|U)一定为0
所以这个互信息 公式9可以简化为公式10 也就是希望脉冲触发的概率接近0.5

所以 目标就是让脉冲的发放概率近似到50%左右  这就需要膜电位有50%小于阈值 50%大于阈值 作者在这里引入了一个膜电位的分位数的概念 设U(q)为U的升序排列的分位数 这里因为50% 所以要接近中为数 即U(0.5-) < Vth U(0.5+) > Vth

假设U为高斯分布 则U(0.5)需要近似于均值Uˉ 即Uˉ ≈ Vth 当(Uˉ-Vth)²为0时信息损失最小 所以IM-Loss以此作为定义 使Uˉ与Vth接近来最小化损失 参考公式11

并且这种损失需要和交叉熵损失LCE相结合 λ为平衡系数

4.2 作者还解释了一下 因为在从头训练模型的时候 发放率率在几层之后很快会变为0 导致信息流失严重 这个IM Loss通过调整脉冲的发放率在0.5左右 可以起到一种类似归一化的效果 在几层之后依旧维持0.5脉冲发放率 参考图2的对比

4.3 由公式7所示 一定存在一个脉冲无法求导的问题  所以作者提出了一个自定义的代理梯度ESG来替换脉冲的梯度 公式13 使用了一个k控制对于膜电位和阈值之间的差值 并使用tanh 参考图3 当k较小时 梯度会比较平滑 反之较大值 只有在x≈Vth的时候幅度才会变大 所以更新范围变窄 梯度下降会变得更精确 参考公式15 通过设定K的上界和下界Kmin = 1 Kmax=10 通过i进行驱动 随着训练步数的增加k逐渐变大 符合前期梯度更新快 后期更新慢但准确的特征 而不是普通SG的固定梯度变化

4.4 展示了作者提出的ESG的效果 参考图4

5. 作者为了适配神经形态硬件 使用平均池化层 并进行预热启动 由IM-Loss训练几次迭代之后的模型作初始化 避免随机初始化的问题 并且进行了消融实验 发现结合tdBN后效果更好 