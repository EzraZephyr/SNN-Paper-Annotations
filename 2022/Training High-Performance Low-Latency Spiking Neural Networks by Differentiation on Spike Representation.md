
## Training High-Performance Low-Latency Spiking Neural Networks by Differentiation on Spike Representation

[论文链接](https://arxiv.org/abs/2205.00459)

将脉冲转换成不同的发放率形式进行训练

首先还是使用经典的IF和LIF模型进行试验公式1 2  同时公式3a-3c表示神经元动态的离散化形式

在这个模型中有一点不同 就是设置Δt远小于LIF的时间常数τ 确保τ具有更大的可行范围 因为离散化的有效性要求τ必须大于Δt
有一些研究直接设置了Δt为1 这样就意味着τ不能为1 限制了取值范围

根据方程4和6 IF模型的SNN脉冲动态可以被描述为公式7 LIF被描述为公式8 其中公式8第二项 使用了离散化抵消 括号内整体变小 传入电流整体变大平衡

公式9描述了反向传播的总过程概述 用同样一组权重 改为发放率 向前计算近似为次可微映射gw 避免了脉冲的不可微

3.3.1推导LIF模型的脉冲表示和次可微映射公式
首先将公式5带入公式6 并设置λ为exp(-Δt/τ) 带入公式6得公式10 再通过n = 1 到N求和得公式11 让I^[N] =,,, a^[N]=,,, 具体看原文 的公式12 设定Δt趋近于0 根据泰勒展开求得 exp(-Δt/τ) 为1-Δt/τ 带回公式12(λ)可以得出发放率的公式为13
这是 因为膜电位跟阈值关联很大 因为膜电位的累计在阈值范围内或者稍高于阈值 超过的话会被重置 所以Vth小了 V[N]一定会小 同时当N趋向正无穷且Δt趋向0的时候∑λN−n≈τ/Δt 所以第二项就可以被化简为V[N] / τ 又因为这一项通常需要整个时间序列的演化 消耗非常大 所以这是可以被接受的精度损失
简化之后 因为发放率一定小于Vth/Δt之间 又不能小于0 所以在时间趋于正无穷时 可得公式14 

对公式14进行多层推广 定义理想发放频率z 每一层由前一层的频率和权重进行计算 当时间趋于正无穷 每一层的实际传递频率a都能近似于理想传递频率z 则证明每一层的发放率可以近似为一个次可微映射zi 这种发放率可以被理解为是带λ权重的发放率

关于为什么是1/τ 从公式8的标准LIF离散形式 通过一些定义得到公式10 然后又通过Δt趋近于0得到公式12 又Vth很小舍弃第二项 得出直接/τ

IF同理 具体见附加材料 离散化后求发放率 让N趋近于正无穷得a[N] ≈ I[N]得到公式15 和proposition一样思路证明

补充一下IF的发放率问题 根据A.2关于公式15的推导 也就是这个IF的脉冲表示和次可微映射公式 这个不是标准的发放率 是使用Vth的缩放发放率 让单位匹配V/s 而不是标准发放率的s^-1

因为后续a要和I等进行比较 参考公式27, 15等
包括量级一致 例如上文说的发放率一定在[0, Vth]之间

3.4 使用链式法则反向传播 SNN输出o为线性的发放率而不是脉冲 所以可以直接求梯度 而且因为发放率是按层输出的 所以无需通过时间域进行反向传播 这样和BPTT方法相比 特别是在时间步数少的时候 提升了训练效率

当然这是理想化的计算 在有限的时间步内 即N不为无穷 还是会存在表示误差 第四节进行优化

3.5 分为两种误差的结合 一种误差为量化误差 主要是因为不完美精度导致的 即只能取离散值 导致离散值之外的理想值和发放值之间的误差
另一种误差为偏差误差 即脉冲发放的时间随机导致的 这种误差作者表示根据统计学 这种误差不会对训练产生什么影响

所以需要深度研究eq误差 以IF模型为例 当所有时间步的输入电流I*保持不变时 缩放的平均发放率可以表示为公式18
a[N] 在IF神经元模型已经被定义为1/N ∑Vth s[n]
这里因为输入电流保持不变 总时间为N θ在图2中表示为阈值 所以s[N]为clamp内的式子

作者提出了两种解决方法
第一种
通过公式18可以观察到 使用较小的脉冲阈值可以减少量化误差 因为Vth/N 但是这也会削弱SNNs的近似能力 所以这里可以使用一种训练阈值的方法 训练每一层的阈值θ和Vth代表一个阈值 参考公式19 使用小批量的优化方法来训练SNNs 根据链式法则 可以对不同批量大小和不同网络模型进行放缩

第二种
第二种方法 在公式3b的基础上对阈值乘上一个参数α 并且把这个参数设置为0.5（实验得出 仅对于IF模型） 同时使用四舍五入计算clamp内的式子 则可以直接将损失减半 参考图2的绿线

experiment省略