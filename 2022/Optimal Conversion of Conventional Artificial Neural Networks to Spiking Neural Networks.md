
## Optimal Conversion of Conventional Artificial Neural Networks to Spiking Neural Networks

[论文链接](https://arxiv.org/pdf/2103.00476)

一种ANN2SNN的方法 提出了阈值ReLU

al+1为ANN的激活值 a`l+1为SNN的发放率 公式1 使用ReLU激活函数 公式2

3.阈值平衡机制 

对于SNN中的第l层 当在t+1收到其输入x'l(t+1)时 将会产生一个临时的电位temp 瞬时值 这个电位是不考虑脉冲触发的 属于是一个积累电位 公式3
这时如果temp电位达到了阈值 则θ=Vth 膜电位减少θ 公式4 否则为0

然后将方程4从1累加到T 并两边同除T 得公式5

然后使用a`l表示平均输入 a'l+1表示平均输出 具体式子看原文 带入公式5化简得到6

如果把阈值的电位设于大于ANN的激活值的最大值的时候 剩余的电位vl(T)一定小于Vth 所以可以用剪裁操作
其中clip式子内 分子根据上文定义展开 表示为Σ(w*x(t))/T 为加权输入的均值 和分母化简可得 (W*x(t))/Vth 可以表示为如果膜电位不考虑发放和重置可以触发的脉冲总数 并且限制一个时间步最多触发一次脉冲 然后将剪裁之后的次数和阈值计算出输出的总强度 随后平均每个时间步计算平均强度

4.误差 

al为ANN的最后一层输出 a'l为SNN的最后一层输出 他们的差值就是转换的误差 将SNN的a'l用ANN的激活函数进行近似 会存在一个Δa‘l 为近似导致的误差 这个误差会随着激活函数种类的变化而变化

总误差为SNN和ANN的输出相减 公式10的第一项 可以看作公式1两个式子相减 因为刚才的公式9算出了激活函数引出的误差Δa'l  将公式9带入这个式子 得到公式10的第二项 然后使用一节泰勒展开式可以近似得到第三项 其中第三项第一部分为激活函数误差 第二部分为前一层的误差通过权重和一阶导数矩阵B传播来的误差

围绕al对L(a`l)进行二阶泰勒展开 带回公式8 可得公式11 其中因为ANN已经被训练好 所以第一项损失Δal可以忽略不计 只计算第二项 将公式10带入展开得公式12 其中交互项2E内表示a’l和a'l-1的相关性 用解耦假设 假设其不相关 交互项被忽略 (可以用Cauchy不等式证明) 然后通过(Botev et al., 2017)这个论文的推导 得H =... 满足递归关系 看原文 将其带回公式12 得公式13的中间式 向前递归得最后一式 然后作者将H矩阵设为常数 则最后的式子可以理解为 总误差为每一层误差的和 所以最小化总误差可以简化为最小化每一层的误差

5.关于如何消除误差

从刚才的那一小结可以的出来总转换误差为ΔL = E||Δa'l||² = ... 看原文
作者先假设了两个极端情况 一种是所有时间步都没有发放脉冲 一种是所有时间步都发放了一个脉冲 这两种极端情况将不会有任何的有用信息传递 所以这里先设置了一个带有阈值的ReLU 即变为min(max(0, x), yth) 这样可以让SNN的范围和ANN的范围对其 可以避免这种情况

然后参考图1b1c 可以看出SNN和ANN的激活函数存在一个系统性偏差 这种方法可以用直接固定一个偏移δ来解决 参考公式14  然后假设输入z为Wla'l-1 并且在区间[(t-1)Vth/T, tVth/T]内均匀分布 又因为公式7 Vth/T为激活阶梯函数h'的输出步长离散化 可以被近似为公式15 导数为0得到最优的偏移Vth/2T 将其带入误差公式后得公式16 为最终的误差函数 即阈值越小 时间越大 误差越小

experiment 测试了ANN2SNN ResNet20的准确率在CIFAR10和100增加了1.28和2.78 超越了原ANN 而且较短的仿真时间让效果更好 VGG16用16个步长就可以和RMP的1536个步长相比 

