
## Online Training Through Time for Spiking Neural Networks

[论文链接](https://arxiv.org/abs/2210.04195)

对于SNN的新训练方法OTTT

首先是使用基础的LIF模型 公式1
公式2是离散化版本的LIF 其中将泄露项整合为1 - 1/τ 来符合生物芯片的机制 同时将正常离散化版本除去衰减前一刻膜电位的项用 R和I使用后面的Σ进行整合 即确保每次乘积只有一个动态数 这样通过移位进行相乘避免浮点

3.2
发放率 使用一个夹紧函数σ控制在0-1之间 因为这里设定了加权平均输入为x* 即在t极限的情况下的输入的平均
则a* = σ(X*/Vth)也一定是一个极限值 所以误差a[t]  - a*有界且随机
发放率可以等于σ内为一种简化表示 忽略了泄露项直接使用类IF模型 用平均加权输入和阈值直接近似发放率 简化计算
根据以上 可以进一部细化为 其前馈网络可以表示成a^l+1[T] ≈ σ((Wlal[T]+b^l+1)/Vth) 具体看原文 为前一层发放率变化之后使用一个Vth进行缩放 因为这里假设的是T足够大 近似为下一层发放率

关于这里为什么要对aN/aN-1依次类推到al+1/W 是因为误差会从输出层aN逐层反向传播到al+1 而Wl的变化又会通过al+1影响到所有后续层的值直到aN 影响损失L 期间aN和aN-1层间的权重保持不变

因为这个a是一个在假设T趋于无限的一个平衡状态 a = σ((Wa* + Fx* + b )/Vth) 从这里可以看出a其实是受自己本身a和自身权重W影响的 这可以被看做成一个循环 即a* = fθ(a*) (后面fθ(a)简写为f(a))这时如果想计算梯度L/θ 用链式法则应为L/a[T] * a[T]*θ 但是a[T]和θ是循环影响关系 无法直接计算 所以用雅可比矩阵对于量化循环的影响来进行计算 即a变化 对应的下一层循环的f(a)变化多少
因为a = f(a)做循环 所以a/θ = f(a)/a * a/θ  + f(a)/θ 其中f(a)/a为雅可比矩阵形式 整理后得到公式L/θ的梯度计算

补充 在循环网络中 链式法则的梯度不仅和f(a)/a * a/θ中的a有影响 还有对于θ本身影响 可以理解为两个参数都可以影响到下一层循环的a 所以这里用全导数公式

标准的BPTT结合代理梯度的反向传播 没有表示反馈连接

因为这个方法不使用代理梯度进行计算 所以基于Heaviside的阶跃函数的脉冲梯度几乎全为0 所以定义公式3 BPTT的标准公式中 累计项内的关于脉冲被近似为0 具体看原文 已给出

这样累积内就只剩下上一个时间步的膜电位和当前时间步膜电位的梯度 文中将这部分用λI代替 且因为这个累计膜电位是按时间进行循环传递 所以最后剩下λ^(t-r) 通过这样化简得到这个公式4的第一部分

随后用gu^l+1[t]代替公式4第一部分的前两项 得到公式4的第二个部分 关于ul+1[τ]/Wl = sl[τ] 参考公式2 对W做偏导之后的结果 但是这里应该是s[τ-1] 不知道为什么是s[τ]

因为这是一个循环网络 存在反馈链接 也就是公式3所省略的 如果正常计算BPTT 这种形式会引入跨层和跨时间的依赖让计算变得特别复杂 需要多时间步和多反馈的传播 所以作者不使用替代梯度 直接用Heaviside阶跃函数的导数近似0的性质 切断了梯度的反馈链接路径上的梯度传播 使得每个时间步独立进行 不需要像公式3和其隐藏的反馈链接一样展开计算

接下来主要是讲如何计算在线梯度，需要每个时间步的瞬时损失 也就是每个t的损失 在正常的SNN中的损失计算中 损失是通过直到最后一个时间步的发放脉冲来进行计算的 而不是通过某一时刻的神经元单独计算 所以作者在之前引入了瞬时损失L[t]   其公式第一项经过链式法则得 可以参考公式5 s[t]/u[t]用替代梯度训练 其中L[t] = ... 允许每个时间步独立计算梯度 使用凸函数时成立 即将所有时间步加起来最后求平均发放率的损失的上界
作者的这个OTTT方法不需要像BPTT那样把整个时间图展开并存下来计算 只需要使用当前步的数据 因此T的大小和内存消耗无关 类似生物学中的“资格痕迹”

证明其有效性 证明OTTT的梯度和基于脉冲表示的梯度可以联系起来 分成三步

第一步 脉冲表示的基本定义a[t]在这里给出 在收敛输入的条件下 l+1层的加权发放率可以近似为al+1[T] ... 定义和上文一样 这时可以发现OTTT中的突触前活动的a^[t]和a[t]非常相似 唯一区别就是一个没用λ归一化一个用了
推导出了基于脉冲表示的梯度 见公式6 可以看出 公式6和公式4 5非常相似 可以将他们联系起来
首先dl+1[T]被定义为 (看原文) 而上文说过 σ在离散条件下是一个夹紧函数 在连续条件下是一个relu函数 将其近似为 sign(....看原文) 这时如果其小于Vth则说明在夹紧范围内导数为1否则为0 而diag(s[t]/u[t]) 不可导 所以使用代理导数近似为(看原文) 所以可以发现其两个导数形式很接近 因为Wlal[T]+bl+1在忽略衰减影响的情况下可以被近似为ul+1[t]

第二步 重新定义Lsrup 最后对比可发现仅相差一个常数项

第三步 于脉冲表示的发放率和OTTT的发放率最后会因为时间步的增加逐渐逼近 所以可以提供下降方向

随后证明了在单层循环网络中的可行性

不使用BN 使用sWS来降低成本

experiments省略