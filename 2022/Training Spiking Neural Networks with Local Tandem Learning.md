
## Training Spiking Neural Networks with Local Tandem Learning

[论文链接](https://arxiv.org/pdf/2210.04532)

提出了一种新的训练方法LTL 让SNN模仿ANN的特征表示 可以让深度SNN的训练更高效 并且解决芯片中的非理想问题

2.1 使用基础的IF和LIF神经元 用离散形式表示 参考公式1到3 其中b表示注入到神经元i的恒定电流

2.2 LTL和普通ANN2SNN不同的是 LTL在神经元级别建立特征表示的等价性 这样可以使其更灵活 并且提出空间局部损失函数 简化了端到端直接训练所需的空间时间分配

其中LTL分为两个版本 一个离线一个在线 离线就是把整个时间窗拿下来一起算 在线就是每个时间步算一次 算完直接更新 不存整个时间窗

首先是离线版本 需要用ANN的激活值和SNN的发放率建立等价性 这里使用MSE损失函数 参考公式4 其中第一项为ANN第l层的归一化激活值 第二项为SNN的第l层的平均发放率 C[t]为总脉冲计数

并使用BPTT来解决时间分配问题
首先计算权重梯度 参考公式5 并且结合公式1和公式2 对其进行链式法则计算 得到公式5的最后一项

同时 U的梯度收到时间的影响 当t还没有到达末时间Tw时 则当前膜电位U会影响下一个时刻的脉冲和膜电位 所以使用全导数公式 参考公式6 即当前层和下一层对于当前层的膜电位影响 参考公式1求偏导可得
当t=Tw 即为最后一个时间步 也就是没有下一时刻可以影响了 所以只需要求当前层的影响(L/S)(S/U)

脉冲对损失的影响 公式7 即公式6被直接写为δ的项 也是分为两种情况 第一种t没有达到Tw 也会造成一个全导数 即脉冲对于总脉冲计数的影响和对下一个时间步的膜电位以及脉冲的影响
(关于脉冲对总计数的影响 (L/C[Tw])(C[Tw]/S)第二项为1 而L/C[Tw] = (L/S[Tw])(S[Tw]/C[Tw])第二项为0且累加 所以为δ[Tw])

无法求导的Heaviside阶跃函数使用boxcar代理梯度实现求导 公式8 通过将该公式带入6和7 可以得到权重的最终梯度形式 其中公式8的超参数p是允许梯度通过的膜电位范围 不同的数据集有不同的p值调整

在离线版本中 使用的是BPTT方法进行展开 这会储存所有时间步的中间状态 导致内存需求很大 无法部署在芯片上

所以提出了“在线版本”  使用局部空间和局部时间进行计算 对于SNN的发放率 使用的是移动平均发放率 也就是Cl[t] = ΣSl[t] 这里可以理解为Cl[t]为l层从1到当前时间t的脉冲总和 这样不会和下一个时间步进行计算 只需要当前时间步已经存在的脉冲信息 每个时间步计算一次 这样不需要储存大面积的中间值在内存里 根据此产生了新的损失函数 其中ANN的激活值不变 参考公式9 为时间步t时刻的“瞬时损失”

同时因为不需要使用全导数了 梯度更新也就简单了很多 参考公式10 根据公式1计算得到 对公式9中的L可以计算ζ项 得公式11

注意在早期的时间步 移动平均发放率因为时间步较小 导致变化较快 会产生很多噪声 所以作者设置了前几个时间步不更新参数 当作"预热期"可以解决此问题

然后作者表明了这种方法相比于HIL训练更适合在芯片上实现 因为HIL需要双向通信 要使用“数模转换器(ADC)” 很贵 而LTL不需要双向通信 因为依赖本地信息 或者可以说像流水线模式 主机提取下一批次的目标 预训练的ANN 然后芯片对当前批次进行SNN训练

3 在CIFAR-10等数据集上测试 5个epoch即可收敛 且精度和ANN接近 并且出现噪声也可以在1epoch恢复