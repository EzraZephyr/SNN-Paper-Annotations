
## Optimized Potential Initialization for Low-latency Spiking Neural Networks

[论文链接](https://arxiv.org/abs/2202.01440)

通过优化初始膜电位优化ANN-SNN的转换

正常的ANN-SNN需要较大的时间步来进行拟合逼近 小的时间步下精度有显著的下降 这篇文章提出了一种优化初始膜电位的方法减少误差

首先公式1介绍了一个基本的ANN模型 ReLU函数激活的 公式2-5是一个基本的IF神经元模型

然后开始进行ANN-SNN的转换 因为转换的根本目标就是让SNN的发放率和ANN的激活值进行拟合 所以首先参考公式6 其将公式5的时间步从1到T进行累加 并除以TVth进行归一化 其中T个时间步的总发放的脉冲除以T时间步就是发放率 又因为x(t) = s(t)Vth 参考公式4 所以公式6可以被替换为公式7

此时 当T特别大的时候 v(T)/T和v(0)/T就可以约等于0 参考公式8 发放率被严格限制在[0,1]之间 同时 对于ANN的激活值也存在一个上界 所以可以用al的最大激活值max{al}对所有激活值进行归一化处理 同样映射到[0,1]之间 参考公式9

同时 按照公式10上面的表示对公式7进行替换 随后比较公式1和公式10 则可以证明当T足够大时 最后两位趋近于0 则ANN和SNN可以直接通过复制权重和偏置进行转换

因为需要较大的T 所以会导致需要消耗的资源增加 而较小的T 会导致ANN喝SNN之间累计产生固有误差 越来越偏离

根据公式10 用一种新的方式重写相邻层脉冲神经元的突后电位之间的关系 对于公式11  其为对于公式10引入floor函数 表示离散脉冲的发放特性
对于公式12 又因为a(T) = r(T)Vth（忽略层l）则可以带入进行求解为公式12的结果

随后假设ANN的第l-1层ReLU激活的al-1与SNN的第l-1层的突触后电位a^(l-1)(T)相同 即（参考原文）。。 然后比较ANN和SNN对于第l层的输出 因为ANN的输出为Wa(l-1)+b 让z等于这个 作为期望平方差的第一项 随后将z带入公式11得期望平方差的第二项  平均转换误差的期望在初始值v(0)为Vth/2时达到最小值 公式14 同时转换误差的期望为0 公式15 这样进行优化 可以实现几乎无误差的ANN-SNN的转换

具体证明看原文（公式13上下文）和Theorem1的公式推导

experiment省略