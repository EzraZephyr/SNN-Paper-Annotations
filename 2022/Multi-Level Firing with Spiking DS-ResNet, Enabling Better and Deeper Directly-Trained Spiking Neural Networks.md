
## Multi-Level Firing with Spiking DS-ResNet: Enabling Better and Deeper Directly-Trained Spiking Neural Networks

[原文链接](https://arxiv.org/pdf/2210.06386)

一种改进直接训练的SNN的方法 MLF和DS-Resnet

4.1
普通的LIF神经元存在一个问题就是无论膜电位大于阈值多少 最后的输出都会为1 这就会导致信息丢失 所以提出了一个MLF单元的概念

MLF由多个LIF组成 每个LIF都有不同的阈值 根据阈值的不同 LIF从小到大排列Vth1, Vth2, ... ,VthK 如果每个阈值的差值为0.1 从1开始 那么一个1.45的膜电位会发放5个脉冲 参考公式4和5 其中oi^t+1,n为第n层第i个MLF单元在时间t+1的脉冲时间序列 然后通过公式6将这个脉冲光时间序列的脉冲整合为一个数 这样就可以区分大膜电位和小膜电位的区别了


反向传播

首先推导基础的梯度
对于脉冲来说 参考公式7 梯度分为空间域和时间域 然后分别展开求梯度 公式8 公式9

对于权重 参考公式10 偏置参考公式11

重点是这里的饱和区域问题 这里对于无法求导的脉冲使用了矩形函数的代理梯度 如果是没有多阈值的MLF单元 只有一个单级发放 那么这个函数的非零区域在[Vth - a/2, Vth + a/2]之间 这会导致膜电位过大的神经元和过小的神经元落在饱和区域 梯度消失 神经元死亡

而使用了多级发放 并且每级的发放不重叠 Vth_(k+1) - Vthk = a 那这样这个函数的非零区域的宽度将会变成[Vth1 - a/2,  VthK + a/2] 范围从a变为Ka 大幅增大 这样就不太可能让膜电位落在饱和区域 对比图2的a和b 这样可以让信息传递的很完善 大幅减少出现梯度消失 信息丢失的情况

4.2

关于一个可以抑制休眠的DS-ResNet

用tdBN和MLF替换 Spiking ResNet的BN层和ReLU层

重点是MLF层的位置 在正常的Spiking ResNet里 激活函数是会在连接加法之后进行总激活的

但是这样在MLF里会大幅提升神经元死亡的概率 因为MLF的输入会被添加一个脉冲1 这在K值较低的情况下影响很大 而如果把MLF层放在连接之前 参考图3的格式 先进行MLF 再进行连接 会保持MLF的防止神经元落入饱和区域的特点 具体的概率证明在附录C 公式C.1到C.3 在K=1的情况下进行的计算

然后因为调整了模块的位置 需要验证一下是否还具有恒等映射能力 附录C 恒等映射有效情况是公式C.4 否则输出会固定为0或者1 失效 通过C.5计算出更改后的DS-ResNet可以达到概率为1.0的恒等映射

experiment省略