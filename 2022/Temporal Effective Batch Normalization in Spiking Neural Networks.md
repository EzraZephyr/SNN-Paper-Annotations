
## Temporal Effective Batch Normalization in Spiking Neural Networks

[论文链接](https://openreview.net/forum?id=fLIgyyQiJqz)

一种新的归一化方法 -- TEBN (时间有效批归一化) 这种归一化通过调整每个时间步的前突触电流分布让时间分布更平滑

3 使用的实验模型为基础的LIF模型 公式1到4 其中公式1显示体现了线性电阻R(通常作为隐变量容易权重里) 在4.4节有讨论

随后介绍了基本的BN 公式5 引入了两个可学习的参数γ和β 公式6

4.1 在传统BN的使用中会面临ICS的问题 也就是随着训练的 空间分布会发生变化 导致优化困难 而这种现象到了SNN中 会因为时间维度上的变化 出现特别大的差异 这样不仅仅停留在空间上 被称为TCS

作者建立了所有时间步的总体期望和方差(μtotal, σ²total)与单个时间步(μ[t],σ²[t])的关系
见公式7 8 其中8为总体方差的展开形式 其中σ = (省略写法)x[t] - μtotal = (x[t] - μ[t]) + (μ[t] - μtotal) 可以分解为单时间步内的偏差 和时间步均值的偏差 继续计算后得到原文公式8的结果

并且假设每个时间步相同 即公式8中的μ相同 可以将最后两项近似为0 得到公式9

4.2 通过刚才公式9的发现 可以将每个时间步的前突触电流的分布视为总体分布的缩放 所以作者提出了TEBN 公式10 11 其中公式12的x^是TEBN的核心归一化公式 并通过时间步的可学习权重p 与全局参数γ和β结合 来调整输入x的分布  让某个样本上的每个时间步的输入分布更均匀   缓解TCS问题 公式13


4.3 损失函数对于权重的参数 参考公式14 15 其中Heaviside阶跃函数造成的不可导使用代理梯度解决

p[t]对于l层的梯度 参考公式16 其中x^ = 第三式括号内的项 其中γ和β也会根据反向传播进行计算

4.4 关于参数p的讨论 作者表示 这部分相当于在LIF的输入RI的基础上(R为电阻) 根据时间维度又添加了一个可学习的"电阻" 解决固定超参数 即在普通LIF模型中不可随训练改变的τ和R

5 通过定理1和2证明了TEBN可以平滑SNN的优化景观 稳定梯度范数 具体看原文

experiments省略