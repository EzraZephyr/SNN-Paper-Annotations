
## Training Spiking Neural Networks with Event-driven Backpropagation

[论文链接](https://proceedings.neurips.cc/paper_files/paper/2022/file/c4e5f4de1b3cfc838eec6484d0b85378-Paper-Conference.pdf)

[附录链接](https://proceedings.neurips.cc/paper_files/paper/2022/file/c4e5f4de1b3cfc838eec6484d0b85378-Supplemental-Conference.pdf)

提出了一个新的核函数(SRM中) 证明了反向传播中使用最大池化优于平均池化

3.1

使用SRM做前向传播 当前层的第i个神经元的t时刻的膜电位 完全由上一层的所有脉冲得来 上一层的所有脉冲都有一个发放时间点   τ 通过这个τ和当前时间步通过核函数ε做对比 如果t-τ的值小 证明他们很接近 影响也就大 反之就小 公式1到3

3.2

证明层间梯度之和的不变性

其中公式4和公式5是由[32]这篇论文得来的近似公式 分别为计算当前层脉冲时间对膜电位的导数 公式4 和膜电位对前一层脉冲时间的导数  公式5
对当前层的脉冲时间和上一层的脉冲时间进行求导 使用链式法则可以分配出公式4和公式5两个式子 对其求积后可以发现其值为1 则可以表明关于这个脉冲时间梯度的传播很稳定 不会在传播中更改 然后计算L对前一层脉冲时间tm的梯度 通过链式法则得到公式7 其中范围为硬性限制 只考虑最近的区间 前一层神经元j的脉冲时间tm只影响之后的脉冲 不会影响到之前的脉冲 且只持续到下一次脉冲tnext 反映了对于前一层的脉冲时间的损失可以通过当前层进行影响 并且保持综合不变 具体查看图2 随后将所有梯度加总可得公式8 意味着梯度之和在此规则下层间不会改变

参考图3 在核大小为k的平均池化中 l层脉冲的梯度会被平均传播到第l-1中的k*k区间内 这个区间内会存在没有发放脉冲的神经元 而这些没有发放神经元无法将信息进一步传播 导致信息丢失 这就会导致梯度之和会发生变化 即不符合刚才证明的前后相邻层梯度不改变的定理 而在最大池化中 第l层的脉冲梯度会被完全的传播到第l-1层中发放脉冲的其中一个最早发送脉冲的神经元 这保持了梯度信息   不会丢失 梯度之和不变

3.3

如果Δt>0 将tm+Δt 向右移动 会导致膜电位增加 进而tk会减少 向左 因为发送脉冲提前 反之如果Δt<0 tm-Δt 向左 膜电位减少 tk发放脉冲的时间就会增加 向右 但是这会导致一个问题 当我们想让其多发放脉冲 即tk向左 需要让上一层tm向右来实现 (假设权重w>0)但是如果上一层tm向右了 那么上上一层 就会可能会向左 这会导致前一层神经元发放出较少的脉冲 连带着反而导致当前层神经元发放出更少的脉冲

更正式的用公式解释的话 参考公式9-11 当ε核函数里的τ过小的时候 对τ求导会得到一个正值 也就是上升阶段 但是当ε大 已经处于下降阶段的时候 对其求导可能会出现负值 所以这出现两种情况 第一种就是ε大 权重w为正 则梯度方向相反 第二种是ε大 权重w为负 则可以将梯度方向转为正方向 但是这种转为正方向也是不对的 由于w为负了 则tm会向左移动 促使tk向右移动 出现问题 所以这种死循环的问题会导致网络出现矛盾无法学习 所以双指数脉冲响应核与事件驱动学习中的时间梯度不兼容 需要一个新的核函数

为了解决上述的问题 将方程4和5给定的近似方程中的∂ϵ(tk-tm)/∂tm替换为一个新函数h(tk-tm) 反向传播公式会被更新为公式12 并且h()乘以任意常数 公式12都不会进行改变 所以避免dετ/dτ带来的符号变化 并且为了传播的时候不被反转 需要确定当t>0时 h(t)需要始终大于0 所以选择h(t) = e...  一个单调递减的正函数 其中τ为一个可调参数 具体式子看原文

3.4 为总体学习规则 用计数损失函数进行计算 公式13推导看附录 14为总梯度公式

通过消融实验验证了作者提出的这个新核比原本的核更好 并且在CIFAR数据集上得到了时间梯度方法的sota
