
## GLIF: A Unified Gated Leaky Integrate-and-Fire Neuron for Spiking Neural Networks

[论文链接](https://openreview.net/forum?id=UmFSx2c4ubT)

提出了一种通过引入门控因子来融合生物学特性的LIF ---- GLIF

首先作者先回顾了一下LIF模型 也就是最基本的离散形式 公式1到3

首先在生物学特征代表了三种关键的神经元行为 以LIF分别是膜电位衰减 整合积累和重置的类型 这几种神经元影响了LIF模型的性能 参考公式1使用τU(t-1,l) 进行了指数衰减 所以τ可以作为一个原语 通过选择不同的原语可以构建不同的LIF变体 但是这样单独的LIF变体是不够的 比如说构建完成之后 一个神经元的原语就不能被改变了

所以 作者提出了GLIF 通过门控功能融合膜电位衰减 整合积累和重置类型的双重特征 控制比例 来增强性能 参考公式4到9

接下来解释三种门控单元

L:

使用了两种衰减方法 第一种为τlin 即线性衰减 第二种为τexp 即指数衰减 通过门控单元Gα控制 即公式10
其中α作为门控因子被训练 限定值在(0,1)范围内 从公式可以看出来当α接近0的时候 公式10的第一项接近1 第二项接近0 则意味着GLIF的线性衰减占优 反之指数衰减占优 通过训练使α找到一个合适的值来平衡线性衰减和指数衰减的幅度 正常情况下α不会取到0或者1 该膜电位衰减为双重原语

I:

依旧使用双原语来进行整合积累 gt表示电导原语表示脉冲权重随时间的影响 l表示基本的均匀编码 参考公式11 同样使用一个可训练的门控因子β用来调整 当β接近0时 均匀编码占优 反之灵活编码占优

F:

关于重置部分也是一样 用一个门控因子控制γ 公式12
需要注意的是 这里的硬重置的参数Lexp又公式10给出 也就说明GLIF的硬重置只有在膜电位指数衰减时有效 当γ接近0时软重置占优 反之硬重置

以上为不取极端值假设 在极少情况 例如αβγ分别为101时 退化为普通LIF 衰减为单原语

该方法将上述所有与膜电位的参数 例如τlin τexp Vre gt Vth和门控因子αβγ这些参数全部用sigmoid函数进行计算 其中xp是一个可训练的参数 范围为-∞到+∞ 通过这种方法将上述参数全部转换为可训练的参数
并且采用了通道级共享 同一通道内的神经元共享一组参数 不同的通道具有不同的参数设置

训练过程中 用BPTT进行反向传播 不可导的Heaviside阶跃函数用公式13代理梯度来解决
随后在3.3节分析了这三种控制的效果 具体看原文

experiment省略