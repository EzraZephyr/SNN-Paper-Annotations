
## Exploring Lottery Ticket Hypothesis in Spiking Neural Networks

[论文链接](https://arxiv.org/abs/2207.01382)

使用LTH训练高稀疏的网络 同时节约搜索成本

首先基于LTH彩票假说 对于ANN有一个EB的概念被提出来 即

在标准的LTH中 目标是找到中奖票 即一个原始网络剪掉不重要的权重性能仍接近原始网络 在基本的LTH中使用的是IMP剪枝方法 每次训练网络到收敛 根据权重的绝对值剪掉一定比例的权重较小的值 生成掩码 然后初始化剪枝过的网络再进行训练 重复 比如说我一次剪20% 那就要完全训练四次以达到剪80%的结果 成本极高

首先是再LTH提出被用于ANN的EB方法 是对于IMP的一种优化方法 每次迭代生成的掩码mk 和前几轮迭代的mk-1 mk-2等进行比较 如果其掩码差异小于一个预设阈值τ 则证明网络稳定 一次性剪枝后重新训练到收敛 这样成本极低 因为这里的训练目标是掩码差异小于一个τ 所以预设的剪裁百分比可能不准确 例如20%的剪裁最后可能会因为求稳而剪裁25% 

但是这种方法如果在SNN的训练中 虽然可以进行非常快的搜索 但是无法实现超过90%稀疏度(关于EB的原始论文中没有提到是否实现大于90的稀疏度 )

所以在3.3中作者提出了一个疑问 就是能否用较短的时间步T`训练的SNN找到使用较长时间步T训练的SNN中重要的权重链接

做了一个实验 首先选择了较短的时间步2到4分别实验 先针对这几个时间步训练一个完整的网络直到收敛 然后剪掉一些绝对值低的权重 重新初始化用时间5进行训练 结果发现当时间步>=3时 剪枝后的网络用时间5重新训练后准确率和没剪枝的网络不相上下

根据刚才的验证 作者提出了一个想法 也就是如何找到一个最小的t 进行彩票剪枝之后还能保留和完整的T相似的准确率

使用KL散度来完成验证 即值越小 两个分布越相似 PT‘为使用t个时间步训练的网络输出的类别概率 PT为同理的完整时间步

需要注意时间步T’的取值需遵循大于1且不等于T 因为取这两个无意义

当PT‘的T’值越接近T时 其KL散度越小 因为累计的输出更接近 所以作者的目标是设置一个阈值λ 当这个KL散度小于这个阈值时就被认为已经足够相似 可以接受 也就是找到一个能接受都范围内最小的T’和T的输出相似

且归一化DKL不改变其本质影响

根据伪代码 在训练两个周期后开始搜索Tearly 直到找到满足Dkl<λ最小的Tearly

experiment省略