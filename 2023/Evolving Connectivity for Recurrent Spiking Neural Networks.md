
## Evolving Connectivity for Recurrent Spiking Neural Networks

[论文链接](https://arxiv.org/pdf/2305.17650)

提出了一种的新的训练框架 EC框架 解决RSNN中训练的问题

3.这里是来自储备池网络的经典RSNN架构 根据Dale定律 一个神经元的所有突触要么全是兴奋性的 要么全是抑制性的 所以网络被分为两个组 一个兴奋性组 一个抑制性组 使用基础的LIF神经元建模 当膜电位u超过阈值时发放一个脉冲 并且将膜电位重置为0 参考公式1 -u为泄露项 其中g={Exc, Inh}表示兴奋和抑制性的组 R表示电阻 c表示突触电流 在没有输入的形况下u成指数衰减
关于这个突触电流c的动态参考公式2 其中第一项 -c/τ为衰减项 τsyn类似膜电位的泄露 Igj表示突触强度 大于0时兴奋性 小于0是抑制性 这跟上面说的Dale定律一致 使用一个dirac函数筛选出来t = tj的瞬时脉冲 Iext表示外部的输入信号 通过观察值x的线性投影获得 例如一些机器人的传感器数据什么的

然后通过Δt进行离散化得到公式3到公式6的差分方程 其中dc为电流的衰减因子 dv为膜电位的衰减因子 具体指数公式看原文 输出向量参考公式7 s^(t-τ,g)模拟了过去τ一段时间内的脉冲状态 k(τ)为加权函数 模拟了将脉冲活动转为一系列任务信号 比如说控制机器人某些动作的指令

但是这种RSNN方法是必须要进行误差反向传播训练的 这种在神经芯片比如说Loihi2等无法支持这种操作 所以作者提出了一个可以绕过误差反向传播的EC框架

4.这跟EC框架主要有三个主要步骤

第一个 重构 首先传统的稀疏连接权重矩阵可以被描述为一个权重矩阵w和一个01掩码θ 用于表示是否存在一个连接 参考公式8 使用ER随机矩阵生成稀疏网络 每条可能的连接都是同伯努利分布B(ρ)中提取的 例如如果ρ=0.5 那么每个连接会有50的概率存在 作者这里收到了深度神经网络中子网络的启发 即使使用随机的权重 也可以找到一个子网络 性能接近原始网络 这样可以证明其实哪些神经元连接了比权重更重要 所以这里作者将目标改为找到一个连接概率矩阵 ρ=⟨ρij⟩ 然后将所有的权重设置为单位大小1 这样Wij就等于θij 参考公式9 网络就会由哪些连接了决定 而不是权重 并且这样01的运算也增加了性能

第二个 优化 上面的重构说了 目标是找到一个最好的ρ 首先参考公式10 R(θ)为一个由连接θ定义的一个总回报 并定义J(ρ)为期望性能 找到一个ρ*以让J(ρ)最大化 也就是最好的ρ 然后为了优化这跟目标 作者使用了NES自然演化策略来优化概率分布参数 主要原因是这里不需要直接计算梯度 而是可以直接通过采样和评估来估计 这对01值脉冲很重要 具体公式参考11到13 这样就通过简单的采样和评估得到了梯度估计 可以按照NES方法在估计的梯度上进行梯度下降 参考公式14的中间式 同时将步长按方差成比例缩放 α = ηVar[B(ρ)] 伯努利分布方差为ρij(1-ρij) 这样在ρij接近0或者1 步长就会很小 反之步长就会比较大 将这跟α和公式13的梯度估计带入 得到最后的式子

然后为了避免ρ变成0或1 这里将ρ限制在了[ϵ, 1-ϵ]的区间内 ϵ为一个很小的正数

第三个 提取最后的参数θ 在上面优化完了之后 在ρ=0.5处设置阈值 大于的θij=1有连接 反之为0

5.EC框架的优点 第一个就是仅推理 因为不需要计算梯度 只需要用NES进行前向推理计算性能R 根据R更新ρ然后更新参数θ 第二个是可扩展 因为每个θ都是独立的 所以可以并行评估 第三就是一位连接 也就是将权重转换为了单位 等于θ 为01值

experiments省略
