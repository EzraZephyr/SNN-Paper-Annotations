
## Spike-driven Transformer

[论文链接](https://arxiv.org/pdf/2307.01694)

提出了一种新的脉冲transformer模型 有四个特性 i事件驱动 ii脉冲通信 iii自注意力(文中提出的一种SDSA)在token和通道维度上有线性复杂度 iv这个SDSA的QKV是掩码和加法 能使得能量消耗低至87.2倍

3这个模型使用最基本的LIF神经元完成 参考公式123，其中U表示膜电位 为上一时刻状态和当前输入 脉冲通过阶跃函数控制 超过阈值发放 如果发放则reset β为衰减因子

3.1模型的整体架构参考图2 包括四个部分 SPS SDSA MLP和分类头

对于SPS部分 遵循论文20的设计 参考公式4 首先将输入I通过一个PSM得到一个一会准备残差的u 再将u传入RPE层 公式5 6 随后和残差u进行连接 公式7 生成U0膜电位

然后再将U0传入SEB块 参考公式8 首先对膜电位进行一次脉冲发放作为输入 然后对前一层的脉冲通过SDSA块(可以通过图看见里面是QKV架构) 和上一层的膜电位生成当前层的中间膜电位 然后发放脉冲S`l 公式9 10 然后进行MLP 和中间膜电位做最终输出 公式11 然后经过全局平均池化 用分类头得到了最终预测结果Y

3.2 残差链接部分 这里使用的是膜电位的残差方法 因为传统的SEW块可能会导致出现非01的脉冲尖峰 而使用膜电位进行可以避免这个问题 始终为二值

3.3关于这个SDSA有两个版本

第一个版本 参考图1b的左侧 首先输入一个脉冲序列 然后进行线性操作 这里的线性操作为仅加法 然后进行脉冲发放 然后 对Q和K进行逐元素相乘 求和 发放脉冲 最后和V再进行一次相乘 得到加权后的张量 参考公式13

然后 逐元素乘积可以交换 所以可以改写为公式14 所以可以看出这其实是一种线性注意力 因为用SN作为核函数 而不是softmax 然后作者假设了一种情况 如果每个头的通道数为1 H=D 那么多头注意力可以简化为对H个头进行分别计算后拼接的结果 参考公式15 这样就不用进行跨通道处理进行逐元素相乘了 而是可以进行单通道点积 这样消耗会更低 这个也就是图1b的右侧 第二个版本

4主要分析了一下为什么上面的一些操作可以实现高效 当二值计算的时候 MLP 输入为0不触发 1只为一个权重加法 注意力机制也一样 当只有二值的时候 逐元素乘法等价于掩码操作或者AND操作 所以没有MAC操作 能量消耗很少

experiment省略