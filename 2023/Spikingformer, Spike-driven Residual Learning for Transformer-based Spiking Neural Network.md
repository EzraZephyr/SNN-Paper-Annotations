
## Spikingformer: Spike-driven Residual Learning for Transformer-based Spiking Neural Network

[论文连接](https://arxiv.org/abs/2304.11954)

一种新的和transformer结合的模型Spikingformer 并且调整了残差学习结构 可以让所有计算都是AC

3.1

公式1 公式2和图1a展示了Spikformer和SEW ResNet的计算方法 其中S  = SNl(ConvBNl(Ol-1)) 这种设计一定会引入非01的脉冲  因为如果残差之前已经输出了脉冲 经过残差块依旧是一个脉冲 那他们连接之后会导致脉冲范围扩展到2 而不是01 这会导致在下一层BN计算中需要使用MAC操作 无法应用于标准神经芯片中 而且这种范围的扩展会根据数据集和增加导致增加 可能达到16

3.2

可以通过调换一下BN和SN的位置解决 图1b 这样可以确保进入BN层的一定是一个从SN输入的01脉冲 而不是2或更大的值 这样可以直接在BN层用AC操作进行计算 符合神经芯片的特性

3.3

首先参考图2 第一部分脉冲Tokenizer将图像输入 进行分块嵌入 然后降采样 参考公式5 从TCHW变为TND

第二部分是transformer块 首先看公式6 为这个transformer块的第一部分 SSA为自注意力处理后的输出 和处理前的输出进行残差连接 为图中transformer的第一部分残差 然后进行MLP 公式7 同样和进行前的进行残差 图2的第二个残差连接

第三部分为分类 参考公式8 用GAP捕捉全局特征 这里还有一种可选的 就是把最后一层的MLP输出再进行一次脉冲计算 再GAP

其中公式9和公式10为脉冲Tokenizer的两种方法 一种不带降采样的SPE 一种是带降采样的SPED

transformer块解释 作者提出了一种纯spike形式的自注意力机制 上面说了 将输入先进行脉冲发放SN后再传入ConvBN可以避免MAC计算 参考公式11 12 这样计算出的QKV就为纯脉冲数据 随后将其相乘计算注意力 并使用一个缩放因子s 参考公式13 这样计算出的记过再通过一层SN和ConBN 还是一个脉冲格式的输出’

分类块解释 一共有四种形式的分类头 公式14到17 作者这里选的是公式14的方法 但是需要浮点乘法的操作 只有先进性脉冲SN 再avg和fc才能避免 公式15的这个 但是需要更多的参数 所以作者取了一个权衡结果公式14

3.4

计算能量消耗 因为卷积的同质性允许后续的BN和线性变化等价融合到带有偏置的卷积层里 所以忽略BN消耗 公式18计算了脉冲的突触数量 融合假设MAC和AC在45nm的硬件上实现 EMAC = 4.6pJ, EAC = 0.9pJ 消耗可以计算为公式19和20 具体看原文

experiment验证了上面说的分类头的问题 和基础数据集测试 具体看原文 省略