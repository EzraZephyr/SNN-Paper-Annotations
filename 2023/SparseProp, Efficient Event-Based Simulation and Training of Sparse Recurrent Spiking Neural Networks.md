
## SparseProp: Efficient Event-Based Simulation and Training of Sparse Recurrent Spiking Neural Networks

[论文链接](https://openreview.net/pdf?id=yzZbwQPkmP)

提出了一种SparseProp的事件驱动算法 利用网络的稀疏性结合相位振荡器的思想 将计算成本从O(N)降到了O(logN)

2.首先考虑一个由N个神经元组成的脉冲递归神经网络的动态 由一组耦合微分方程描述 参考公式1 其中F(Vi)为膜电位再没有外部输入的情况下的动态 比如LIF 那就是衰减 I为外部输入 最后为递归输入 是其他神经元对i的影响 Jij为i和  j之间的连接突触的强度 分为兴奋和抑制性 h为一个时间核 可以理解为捕捉tj的瞬时输入 所以只有当j神经元在t时刻发放脉冲了 最后一项才会被驱动

这种方式模拟的网络设计4个简单步骤 1.找到下一个发放的神经元j* 和发放时间tj* 2.将每个神经元的状态 Vi 演变到下一个网络脉冲时间 tj* 3.使用Jij的权重将突触后神经元i更新为j* 4.重置发放脉冲的神经元Vj* = Vreset

这样基于事件的模拟相比于ODE的预估-矫正迭代的方法 这种需要计算所有小步长 例如Δt为0.1 每个时间步长更新N个神经元的状态 这样会计算出大量的无用结果 其总累积误差为O((Δt)^p) 其中p为ODE的阶数 可以看出Δt越小精度越高 计算成本越大 而基于事件的模拟的只在脉冲时刻更新就避免了这个问题

2.1首先 作者用简单的神经元模型 例如LIF QIF将其变化映射到一个相位振荡器模型 假定输入I恒定 参考公式2 其中相位ϕ代表到下一次发放脉冲的距离 这个相位变化是一个线性变换 只代表了距离脉冲的距离 而不是那种需要指数计算衰减的膜电位

g(ϕ)为一个相位转移曲线 当神经元i作为j*的突触后神经元 i会受到j*发放的影响 可能是抑制也可能是兴奋输入 所以需要g来做这种调整 当没有受到其他神经元影响的时候 相位ϕ会以速度w进行一个线性增加 参考图1c 所以 在相同的速度w的情况下 一定是相位ϕj*(ts)最大的神经元最接近发放 所以下一次脉冲事件t(s+1)的公式就是tj* = (ϕth - ϕj*)/w 但是这种传统的方法 每次需要计算所有神经元的相位更新 计算复杂度为O(N)

然后作者在这里引入了一个全局偏移 参考公式3 记录所有神经元经过w线性变化后的增量 在初始条件Δϕ = 0 t =0时 不用像上文所说一样每次更新所有神经元的相位 只需要更新一个Δϕ 也就是先假设其没有其他层神经元输入 因为在没有神经元输入的时候 且I固定 那Φ的增长速率一定是一样的 所以只关心累计的距离 然后再通过g进行调整

然后在抑制主导的网络中 输入会让相位ϕ越来越负 这样累积之后Δϕ就会产生一种灾难性消除的误差 具体看附录C 在一个大数减去一个特别小的数的时候 会因为精度限制导致这个减法被消除 所以设定在Δϕ超过某个阈值Δthϕ时重置以避免

3.计算成本
SparseProp每次的网络脉冲的分摊成本按O(Klog(N))缩放 因为N为神经元数量 K为突触连接 作者使用了一个包含10的六次方个神经元和k=100个突触网络 持续100秒 需要不到一个CPU小时 而传统方法需要超过三个CPU年 具体查看表1

4.1根据上文的理论 带入LIF举一个例子 对于一种脉冲耦合的LIF模型 可以被表示为公式4 当膜电位Vi达到设定的阈值Vth=0时 发放脉冲后重置为-1 在两个脉冲之间被解为公式5 设置相位ϕi的范围为(-∞, 0] ϕi为0时发放脉冲 为了定义Φ的线性演化规律 需要先知道在没有其他层发放影响的时候 只有一个稳定的外部输入I时 达到阈值所需要的时间为Tfree 将t=Tfree Vi(Tfree)=Vth代入方程5 化简可得公式6 然后将设设定的Vth=0 Vre=-1带入到公式6得公式7 为神经元没有其他层影响的时候从重置到阈值的时间 然后关于如果有其他层影响的相位转移曲线g看原文 公式8和上面的公式定义 其中c为耦合强度 其中J按照√k缩放 具体原因参考附录E

4.2 关于QIF 因为他的膜电位为V² 所以相对于LIF的生成机制更复杂一些 具体看原文

Experiments省略
