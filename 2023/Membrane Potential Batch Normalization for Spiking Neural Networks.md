
## Membrane Potential Batch Normalization for Spiking Neural Networks

[论文链接](https://openaccess.thecvf.com/content/ICCV2023/papers/Guo_Membrane_Potential_Batch_Normalization_for_Spiking_Neural_Networks_ICCV_2023_paper.pdf)

在传统的SNN中 归一化通常在卷积之后 但是忽略了归一化后的膜电位增量对膜电位的扰动问题 所以作者提出了一种归一化方法MPBN 放在膜电位更新之后 脉冲发放之前

3.1 使用基础的LIF模型 参考公式1 其中u表示膜电位 c表示突触前输入 Wx(t+1)表示在t+1时刻的加权输入 τ为一个衰减项 在(0,1)之间 当膜电位超过阈值时 将会发送一个脉冲 膜电位重置为0 如果没有超过 将会把膜电位传递到下一个时间步 参考公式2

SNN的分类器 在SNN的分类输出阶段 因为输出时01二值的 使用softmax会很困难 而且使用LIF计算为二值脉冲的华会导致丢失很多信息 没法精确的表明概率分布 所以直接用输出层的输入x来进行计算 参考公式3 这一cout将会是一个连续值 最后的oout就可以输入到softmax生成概率分布

3.2 SNN中的BN BN作为CNN训练中的重点直译 可以缓解梯度消失或爆炸的问题 在SNN中应用 可以考虑一个输入c = {c1, c2, ..., cT}的脉冲神经元 参考公式4 μi和σ²i分别表示均值和方差 ε是一个很小的常数防止除0 然后为了可以表示恒等变化 参考公式5 当λ为公式4的分母 β为μi时 相当于没有进行BN

4.1开始介绍膜电位批量归一化MPBN 首先根据刚才说的 虽然数据在卷积层后已经被BN 但是在归一化之后 作为增量加到膜电位上 膜电位还是会出现大幅的扰动 参考公式6 可以很明显的看出来 只归一化膜电位的增量x 和原始膜电位相加之后还是会被扰动 所以 在膜电位更新后 脉冲发放前 再进行一次归一化 但是这种归一化会存在一些不符合生物学原理的情况 就是一些本身没有超过阈值的膜电位 在经过MPBN后可能会超过阈值 反之 可能会小于阈值 而且因为多了一层BN 所以开销会增大

4.2 作者在这里提出了重新参数化 首先参考公式8 这个是使用MPBN之后的触发函数 将MPBN展开之后得到公式9 然后 将其折叠刀阈值中 得到10 这样将共享的Vth变成通道级的Vth 每个通道享有不同的脉冲 这样 在推理阶段只需要正常和已经更新好的阈值进行比较 不需要再进行额外计算 同时也提升了多样性

4.3 在反向传播阶段使用的是循环依赖的STBP 参考公式11 这就会存在一个经典的SNN问题 即脉冲不可导 所以需要对这部分进行代理梯度的计算 参考公式12 使用一个简单的STE代理梯度进行计算

4.4 并且经过消融测试 带MPBN的准确率总是高于不带MPBN的

4.5 同时参考图3 可以看出MPBN的损失景观比不带MPBN的损失景观要平坦一些 优化更稳定

experiments省略