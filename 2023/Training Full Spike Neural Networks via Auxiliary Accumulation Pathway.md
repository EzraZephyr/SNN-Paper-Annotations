
## Training Full Spike Neural Networks via Auxiliary Accumulation Pathway

[论文链接](https://arxiv.org/pdf/2301.11929)

提出了一种双流训练的方法 可以解决全脉冲的信息损失和优化目前高性能SNN的大量MAC操作

3.1

介绍了两个基本的神经元模型 一个IF一个LIF 一个没有衰减项一个有衰减项 公式1到公式5

3.2

在全脉冲的SNN中 所有的计算都使用AC 而ANN中使用的都是MAC 所以随着网络加深 SNN和ANN的消耗能量比值应该是一个趋于稳定的值 参考公式6 其中fr是平均发放率

但是 全脉冲的SNN是一种理想状态 目前很多sota的SNN引入了一些非脉冲操作 比如说SEW块的ADD 这种操作会让脉冲值变为2或者更大 这样在ConvBN中就必须要使用MAC得以计算 因此 更加准确的SNN消耗应该为公式7 其中Emax * Omax为非脉冲MAC操作的能耗

3.3

目前比较主流的两种残差块中 在较早期比较流行的是将ReLU层替换为神经元发放脉冲 这样就构建了一个全脉冲类型 但是这种会导致梯度消失
作者这里假设用IF神经元 可以更简单的实现恒等映射 然后其第l+k-1个残差块对于其l层的脉冲的梯度可以计算为公式8
从第l层一层层传递过来 参考第二项 然后因为阶跃函数不可导 用Sigmoid激活函数的话 这种梯度消失就很容易发生
因为Vth<=1 s为0或者1 这样sigmoid会始终小于1 经过多层传递后会指数级衰减后消失

第二种方法是ADD 参考公式9 对上一层的脉冲输入再做一次脉冲映射 随后对两项相加 这种操作可以实现恒等映射并且可以解决梯度消失的问题 但是如果两项全部发放了脉冲 其结果会为2 是一个非脉冲信号 者在后续就不避免的引入MAC操作 而且网络越深 这个值可能会到10以上 消耗将会更高

4.1

作者在这里提出了一个双流SNN来解决上述问题

首先和基本残差不同的是这里用了一个双路径来表示 参考公式10 其中第一条路径可以看作一个基本的ADD 第二条路径 残差和本身输出的基础上有一个g逐元素函数 其操作查看表1
并且为了防止信息的丢失 需要对其输出的脉冲进行累加 参考公式11 和图1b d
然后 这种累计会存在1+1=2的操作 因此需要用到MAC计算 但是因为其经过ConvBN了  而且降采样次数是固定的 所以这种累计次数基本是固定的 不会是一个很大的MAC开销

然后在训练的时候 为了防止深层的梯度消失 会用辅助累计和正常的路径一起计算损失 参考公式12

4.2

然后恒等映射方面 辅助累计a的恒等映射比较容易 让s为0就可以 所以让f中最后一个BN层的权重和偏置为0就可以实现 在神经元方面 使用IF神经元 因为其没有泄露项 设置0<Vth<=1, Vt-1=0则可以确保ot=1时H>=Vth ot=0时Ht<Vth 实现恒等映射

4.3

关于梯度不会消失的证明 首先是辅助累积 刚才恒等映射说了 当s为0的时候为恒等映射 参考公式11 会是一个常数1 所以不会存在梯度消失的问题

然后是神经元的残差 用一个带辅助累积的ResNet 其第l+k-1个残差块的输出对于第l个残差块输入的梯度参考公式13 关于al+k-1/al在上面已经证明过为1 而o/s为第三式阶跃函数内 所以最后的结论 无论阶跃函数为多少 其梯度为1或者2 都不会消失

然后是逐元素块g的残差 参考公式14 逐层计算 为最后四个式子 很明显 当为AND时 设置s为1克服梯度消失 其余的IAND OR XOR 设置s为0 客服梯度消失

experiments看原文 省略